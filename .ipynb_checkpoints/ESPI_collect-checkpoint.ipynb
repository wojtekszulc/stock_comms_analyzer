{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESPI Collect\n",
    "\n",
    "This module will collect ESPI reports from GPW (Giełda Papierów Wartościowych) website and save relevant information in html and csv format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define functions for data parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import HTML, display\n",
    "import re\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse reporting date and return format yyyymmdd\n",
    "def parse_date(soup):\n",
    "    creation_date = soup.find('td',text=re.compile(r'Data sporz')).next_sibling.next_sibling.text.strip()\n",
    "    creation_date = creation_date.replace(\"-\",\"\")\n",
    "    return creation_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse name of the reporting company\n",
    "def parse_name(soup):\n",
    "    name = soup.find('td',text=\"Nazwa emitenta\").next_sibling.next_sibling.text\n",
    "    name = name.strip().upper()\n",
    "    name = name.replace('\"', '')    \n",
    "    name = name.replace(\" W RESTRUKTURYZACJI\", \"\")\n",
    "    name = name.replace(\" W UPADŁOŚCI LIKWIDACYJNEJ\", \"\")\n",
    "    name = name.replace(\" W UPADŁOŚCI UKŁADOWEJ\", \"\")\n",
    "    name = name.replace(\" W UPADŁOŚCI\", \"\")\n",
    "    name = name.replace(\"SPÓŁKA AKCYJA\", \"SA\")\n",
    "    name = name.replace(\"SPÓŁKA ACYJNA\", \"SA\")\n",
    "    name = name.replace(\"SPÓŁKA AKCJNA\", \"SA\")\n",
    "    name = name.replace(\"SPÓLKA AKCYJNA\", \"SA\")\n",
    "    name = name.replace(\"SPÓŁKA AKCYJNA\", \"SA\")\n",
    "    name = name.replace(\"S. A.\",\"SA\")\n",
    "    name = name.replace(\"S.A.\",\"SA\")\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get text from the main part of the ESPI report\n",
    "def parse_url(soup):\n",
    "    contents = soup.find(\"div\", {'class':'dane'})\n",
    "    # Index of the report\n",
    "    section_index = contents.find_all(\"table\", {'class':'nDokument'})[0] \n",
    "    # Footer of the report - information about the reporting company\n",
    "    section_footer = contents.find_all(\"table\", {'class':'nDokument'})[1] \n",
    "    link_raport = section_index.find(\"a\")['href']\n",
    "    link_raport = link_raport[link_raport.find('#')+1:]\n",
    "    start = contents.find('a',{'name':link_raport})\n",
    "    content_table = start.parent.next_sibling.next_sibling\n",
    "    contents_nTekst = content_table.find_all(\"tr\", {'class':'nTekst'})\n",
    "    total = [(el.text) for el in contents_nTekst]\n",
    "    return ''.join(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_url_and_save(soup):\n",
    "    contents = soup.find(\"div\", {'class':'dane'})\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Collect the ESPI reports from GPW website and save relevant information to htmls files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:13.117693\n"
     ]
    }
   ],
   "source": [
    "start_time = (datetime.now())\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.set_page_load_timeout(30)\n",
    "    \n",
    "espi = []\n",
    "# The range of the id of reports to be scraped from gpw website \n",
    "for el in range(309999,310000,1):\n",
    "    url = 'https://www.gpw.pl/komunikat?geru_id='+str(el)\n",
    "    if os.path.exists(\"htmls/\"+str(el)+\".html\")==False:\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html)\n",
    "            with open(\"htmls/\"+str(el)+\".html\", \"w\") as file:\n",
    "                file.write(str(parse_url_and_save(soup)))\n",
    "        except:\n",
    "            1==1\n",
    "            # print(\"Error at id number: \"+str(el))\n",
    "\n",
    "driver.close()\n",
    "\n",
    "end_time = (datetime.now())\n",
    "print(end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Parse data from the scraped html files and save it to csv file to be used for further processing and modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To process ESPI files it took: 0:15:35.733726\n",
      "Total number of files processed: 23447\n",
      "Total number of ESPI information processed: 22333\n",
      "Total number of errors: 1114\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "path = './htmls'\n",
    "files = []\n",
    "# r=root, d=directories, f=files\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.html' in file:\n",
    "            files.append(os.path.join(r, file))\n",
    "files.sort()\n",
    "\n",
    "dates=[]\n",
    "names=[]\n",
    "espi_filename=[]\n",
    "espi_info=[]\n",
    "espi_errors=[]\n",
    "errors=0\n",
    "\n",
    "for f in files:\n",
    "    url = str(f)\n",
    "    try:\n",
    "        soup = BeautifulSoup(open(f), \"html.parser\")\n",
    "        soup_text = parse_url(soup).lower()\n",
    "        dates.append(parse_date(soup))\n",
    "        names.append(parse_name(soup))\n",
    "        espi_info.append(soup_text)\n",
    "        espi_filename.append(f)\n",
    "        \n",
    "    except:\n",
    "        espi_errors.append(f)\n",
    "        errors+=1\n",
    "\n",
    "output = pd.DataFrame(list(zip(espi_filename,dates,names,espi_info)), \n",
    "                      columns=['filename','date','name','file_content'])\n",
    "output.to_csv('espi_data.csv', index=False)\n",
    "\n",
    "output_errors = pd.DataFrame(list(zip(espi_errors)), \n",
    "                      columns=['filename'])\n",
    "output_errors.to_csv('espi_errors.csv', index=False)\n",
    "\n",
    "end_time = datetime.now()\n",
    "time_taken = end_time-start_time\n",
    "print(f'To process ESPI files it took: {time_taken}')\n",
    "print(f'Total number of files processed: {len(files)}')\n",
    "print(f'Total number of ESPI information processed: {len(espi_info)}')\n",
    "print(f'Total number of errors: {errors}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
